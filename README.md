# web-scraping

Data Collection and Web Scraping

## ğŸ“Œ Overview

This project demonstrates a complete data science pipeline from web scraping to exploratory data analysis. It collects book data from books.toscrape.com, cleans and preprocesses the data, and performs comprehensive exploratory analysis to uncover insights.

## ğŸ“‚ Project Structure

```bash
web-scraping/
â”œâ”€â”€ .venv/                         # Virtual environment
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                       # Raw scraped data
â”‚   â”‚   â”œâ”€â”€ scraped_books.csv
â”‚   â”‚   â””â”€â”€ scraped_books.json
â”‚   â””â”€â”€ preprocessed/              # Cleaned and processed data
â”‚       â”œâ”€â”€ cleaned_books.csv
â”‚       â”œâ”€â”€ cleaned_books.json
â”‚       â””â”€â”€ outliers_info.json
â”œâ”€â”€ outputs/                       # Visualization outputs
â”‚   â”œâ”€â”€ correlation_matrix.png
â”‚   â”œâ”€â”€ price_analysis.png
â”‚   â”œâ”€â”€ rating_distribution.png
â”‚   â”œâ”€â”€ availability_pie.png
â”‚   â”œâ”€â”€ price_vs_rating.png
â”‚   â””â”€â”€ price_vs_availability.png
â”œâ”€â”€ notebooks/                     # Jupyter notebooks
â”‚   â””â”€â”€ eda_analysis.ipynb         # Task 3: EDA notebook
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ web_scraper.py             # Task 1: Web scraping script
â”‚   â””â”€â”€ data_cleaning.py           # Task 2: Data cleaning script
â”œâ”€â”€ logs/                          # Application logs
â”‚   â””â”€â”€ scraping.log
â”œâ”€â”€ .gitignore                     # Git ignore rules
â”œâ”€â”€ requirements.txt               # Python dependencies
â””â”€â”€ README.md                      # Project documentation
```

## ğŸš€ Project Tasks

## Task 1: Data Collection and Web Scraping

### Features

- **Robust Web Scraping**: Extracts book information including title, price, rating, availability, and description
- **Pagination Handling**: Automatically navigates through multiple pages
- **Error Handling**: Comprehensive error handling and logging
- **Respectful Scraping**: Includes delays between requests to avoid overwhelming the target website
- **Multiple Output Formats**: Saves data in both CSV and JSON formats

### Output

- data/raw/scraped_books.csv - Raw data in CSV format

- data/raw/scraped_books.json - Raw data in JSON format

## Task 2: Data Cleaning and Preprocessing

### Description: Clean and preprocess the raw dataset to make it suitable for analysis.

### Objectives Achieved:

âœ… Missing Data Handling: Imputed missing values using appropriate strategies

âœ… Outlier Detection: Identified and removed outliers using IQR method

âœ… Categorical Encoding: Converted categorical variables using label encoding and one-hot encoding

âœ… Data Normalization: Standardized numerical features using z-score and min-max scaling

### Output:

- data/preprocessed/cleaned_books.csv - Cleaned data in CSV format

- data/preprocessed/cleaned_books.json - Cleaned data in JSON format

- data/preprocessed/outliers_info.json - Outlier analysis report

## Task 3: Exploratory Data Analysis (EDA)

### Description: Perform comprehensive exploratory data analysis to understand the underlying structure and trends in the data.

### Objectives Achieved:

âœ… Summary Statistics: Computed mean, median, variance, and distribution metrics

âœ… Data Visualization: Created histograms, scatter plots, box plots, and correlation matrices

âœ… Correlation Analysis: Identified relationships between numerical features

âœ… Insight Report: Generated comprehensive summary of key findings

### Visualizations Created:

- Price Distribution - Histogram and box plot of book prices

- Rating Distribution - Bar chart of book ratings frequency

- Availability Analysis - Pie chart of stock availability

- Price vs Rating - Comparative analysis of pricing by rating

- Correlation Matrix - Heatmap of feature relationships

- Price vs Availability - Violin plot showing price distribution by stock status

## Output:

- outputs/ folder containing all visualizations

- Comprehensive insights report in the Jupyter notebook

## ğŸ› ï¸ Setup & Installation

1.  **Clone the repository** (if applicable) or ensure you have the project structure locally.
2.  **Navigate to the project root directory** in your terminal.
3.  **Create a virtual environment** (recommended):
    ```bash
    python -m venv .venv
    ```
4.  **Activate the virtual environment**:
    - On Windows:
      ```bash
      .venv\Scripts\activate
      ```
    - On macOS/Linux:
      ```bash
      source .venv/bin/activate
      ```
5.  **Install dependencies**:

    ```bash pip install -r requirements.txt

    ```

## ğŸ“Š Usage

1. Run Task 1: Web Scraping

   ```bash python src/web_scraper.py

   ```

2. Run Task 2: Data Cleaning

   ```bash python src/data_cleaning.py

   ```

3. Run Task 3: Exploratory Data Analysis

   ```bash jupyter notebook notebooks/eda_analysis.ipynb

   ```

## ğŸ”§ Technologies Used

- Web Scraping: BeautifulSoup4, Requests

- Data Processing: Pandas, NumPy

- Data Cleaning: Scikit-learn

- Visualization: Matplotlib, Seaborn

- Analysis: SciPy, Jupyter Notebook

## ğŸ¯ Next Steps

This cleaned and analyzed dataset is now ready for:

- Machine learning model development

- Price prediction algorithms

- Recommendation systems

- Inventory optimization analysis
